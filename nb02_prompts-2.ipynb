{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71262cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain langchain-groq python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682c4c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb1d276",
   "metadata": {},
   "source": [
    "# Example 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e461e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "model = init_chat_model(\"llama-3.3-70b-versatile\", model_provider=\"groq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca18e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"Analyze the following code and provide suggestions for improvement:\n",
    "Code: {code}\n",
    "Language: {language}\n",
    "Focus on: {focus_area}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca177aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = prompt_template.format(code = \"def add(a, b): return a + b\", language=\"Python\", focus_area=\"performance and readability\")\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f93f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A simple model call\n",
    "\n",
    "response =  model.invoke(prompt)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044667ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4fd624",
   "metadata": {},
   "source": [
    "# Exercise: A RAG Prompt (Retreival Augmented Generation)\n",
    "\n",
    "https://smith.langchain.com/hub/rlm/rag-prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959ec4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"You are a culinary expert helping with recipe recommendations. Use the available ingredients and dietary preferences to suggest a suitable recipe. If you cannot recommend something with the given ingredients, suggest alternatives. Keep your recommendation detailed but concise.\n",
    "Available Ingredients: {ingredients} \n",
    "Dietary Preferences: {preferences}\n",
    "Cuisine Type: {cuisine}\n",
    "Recipe Suggestion:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15482b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = prompt_template.format(\n",
    "    ingredients = \"tomatoes, onions, garlic, olive oil, pasta\", \n",
    "    preferences = \"vegetarian\",\n",
    "    cuisine = \"Italian\"\n",
    ")\n",
    "\n",
    "from langchain.chat_models import init_chat_model\n",
    "model = init_chat_model(\"llama-3.3-70b-versatile\", model_provider=\"groq\")\n",
    "\n",
    "response = model.invoke(prompt)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aabbd227",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise time:\n",
    "\n",
    "# Change the context to your liking\n",
    "# Ask relevanent question to your context\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35ab1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tech_context = \"\"\"OpenAI announced its latest breakthrough in artificial intelligence with the release of GPT-5, claiming significant improvements in reasoning capabilities and reduced hallucinations. The model demonstrates enhanced performance across coding, mathematics, and creative writing tasks.\n",
    "\n",
    "The company revealed that GPT-5 uses a new training methodology called \"Constitutional AI\" which helps the model better understand context and provide more accurate responses. Early testing shows a 40% improvement in coding accuracy and 60% better performance on complex reasoning tasks compared to GPT-4.\n",
    "\n",
    "Microsoft, OpenAI's primary investor, plans to integrate GPT-5 into its entire suite of products including Office 365, Azure cloud services, and Windows operating system. This integration is expected to roll out gradually over the next six months.\n",
    "\n",
    "Industry experts predict this advancement will accelerate automation across various sectors, particularly in software development, data analysis, and content creation. Some estimates suggest that GPT-5 could automate up to 30% of current programming tasks.\n",
    "\n",
    "The announcement has sparked renewed debate about AI safety and the need for regulatory frameworks. Several tech leaders have called for increased oversight and ethical guidelines for AI development and deployment.\n",
    "\n",
    "Competition in the AI space intensifies as Google prepares to launch its competing Gemini Pro model, while Anthropic continues development on Claude 3. The race for AI supremacy shows no signs of slowing down as companies invest billions in research and development.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b7af5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = prompt_template.format(\n",
    "    ingredients = tech_context, \n",
    "    preferences = \"What are the key technological improvements in GPT-5?\",\n",
    "    cuisine = \"\"\n",
    ")\n",
    "\n",
    "from langchain.chat_models import init_chat_model\n",
    "model = init_chat_model(\"llama-3.3-70b-versatile\", model_provider=\"groq\")\n",
    "\n",
    "response = model.invoke(prompt)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912495fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"You are a technology analyst providing insights on AI developments. Use the following information to answer questions about AI trends and market implications. Provide clear and analytical responses.\n",
    "Information: {info} \n",
    "Question: {question}\n",
    "Analysis:\"\"\"\n",
    "\n",
    "prompt = prompt_template.format(\n",
    "    info = tech_context, \n",
    "    question=\"What impact will this have on the job market for software developers?\"\n",
    ")\n",
    "\n",
    "from langchain.chat_models import init_chat_model\n",
    "model = init_chat_model(\"llama-3.3-70b-versatile\", model_provider=\"groq\")\n",
    "\n",
    "response = model.invoke(prompt)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645eb5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise:\n",
    "\n",
    "# How to put \n",
    "# context = content of a local file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24aed7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"news_article.txt\", 'r') as file:\n",
    "    new_context_from_file = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ac2442",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"You are a literary critic analyzing written works. Based on the provided text, offer insights about themes, writing style, and literary significance. Keep your analysis focused and scholarly.\n",
    "Text: {text} \n",
    "Analysis Focus: {focus}\n",
    "Literary Analysis:\"\"\"\n",
    "\n",
    "prompt = prompt_template.format(\n",
    "    text = new_context_from_file, \n",
    "    focus = \"What are the main themes and narrative techniques used in this work?\"\n",
    ")\n",
    "\n",
    "from langchain.chat_models import init_chat_model\n",
    "model = init_chat_model(\"llama-3.3-70b-versatile\", model_provider=\"groq\")\n",
    "\n",
    "response = model.invoke(prompt)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25539f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise:\n",
    "# Grab a book from https://www.gutenberg.org/ebooks/bookshelf/657\n",
    "# save it as a local text file.\n",
    "# pass that book content as context\n",
    "# Ask questions relevant to the book\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e794ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"book.txt\", 'r') as file:\n",
    "    book_context = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de9951e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3bd05f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
    "Question: {question} \n",
    "Context: {context} \n",
    "Answer:\"\"\"\n",
    "\n",
    "prompt = prompt_template.format(\n",
    "    context = book_context[:len(book_context)//40], \n",
    "    question=\"What is this book about?\"\n",
    ")\n",
    "\n",
    "from langchain.chat_models import init_chat_model\n",
    "model = init_chat_model(\"llama-3.3-70b-versatile\", model_provider=\"groq\")\n",
    "\n",
    "response = model.invoke(prompt)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca643860",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
